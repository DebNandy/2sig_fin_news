{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kaggle.competitions import twosigmanews\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import xgboost\n",
    "from sklearn.metrics import explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = twosigmanews.make_env()\n",
    "(market_data_orig, news_data_orig) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping the originals as is\n",
    "market_data = market_data_orig.copy()\n",
    "news_data = news_data_orig.copy()\n",
    "asset_code_name_map = market_data.set_index('assetCode').to_dict()['assetName']\n",
    "asset_name_code_map = market_data.set_index('assetName').to_dict()['assetCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Level 1\n",
    "def preprocess_news_data(news_data):\n",
    "    news_data['firstMentionLoc'] = 1- news_data['firstMentionSentence']/news_data['sentenceCount']\n",
    "    news_data['sentimentWordFraction'] = news_data['sentimentWordCount']/news_data['wordCount']\n",
    "    news_data = news_data.drop(columns = ['noveltyCount12H', 'noveltyCount24H',\n",
    "                                          'noveltyCount3D', 'noveltyCount5D',\n",
    "                                          'noveltyCount7D', 'volumeCounts12H',\n",
    "                                          'volumeCounts24H', 'volumeCounts3D',\n",
    "                                          'volumeCounts5D', 'volumeCounts7D',\n",
    "                                          'bodySize', 'audiences', 'subjects',\n",
    "                                          'takeSequence', 'headline', 'sourceId',\n",
    "                                          'firstCreated', 'sourceTimestamp',\n",
    "                                          'firstMentionSentence', 'sentenceCount',\n",
    "                                          'sentimentWordCount', 'wordCount','companyCount',\n",
    "                                          'sentimentClass','headlineTag' ])\n",
    "#Think about adding sentimentClass,headline and using it as a categorical variable\n",
    "    news_data['time'] = pd.to_datetime(news_data['time']).dt.date.astype(str)\n",
    "    news_data = news_data.groupby(['time','urgency', 'provider',\n",
    "                                   'marketCommentary', 'assetCodes','assetName'\n",
    "                                  ],as_index=False).agg([np.mean,np.min,np.max]).reset_index()\n",
    "    news_data.columns = [''.join(c) for c in news_data.columns]\n",
    "    news_data['assetCodes'] = news_data['assetCodes'].astype(str)\n",
    "    news_data['assetCodes'] = news_data['assetCodes'].map(lambda x: eval(x.replace('{','[').replace('}',']')))\n",
    "    return news_data\n",
    "\n",
    "def group_providers(news_data):\n",
    "    provider_data = pd.DataFrame(news_data.provider.value_counts())\n",
    "    provider_data.reset_index(inplace=True)\n",
    "    provider_data['map'] = provider_data['index'].astype(str)\n",
    "    provider_data.loc[provider_data.provider < provider_data['provider'][2], 'map'] = 'other'\n",
    "    provider_data.drop(columns=['provider'],inplace=True)\n",
    "    news_data = pd.merge(news_data,provider_data,how='left',left_on='provider', right_on='index')\n",
    "    news_data.provider = news_data.map\n",
    "    news_data.drop(columns=['index','map'],inplace=True)\n",
    "    return news_data\n",
    "\n",
    "def preprocess_asset_code_news_data(news_data,asset_code_name_map,asset_name_code_map):\n",
    "    #Preprocessing Level 2\n",
    "    news_data['assetCodes'] = news_data.apply(lambda row : [c for c in row.assetCodes if c in asset_code_name_map],axis=1)\n",
    "    news_data['countassetCodes'] = news_data.apply(lambda row: len(row.assetCodes), axis=1)\n",
    "    single_code_data = news_data[news_data.countassetCodes == 1]\n",
    "    single_code_data['assetCode'] = single_code_data.apply(lambda x: x.assetCodes[0],axis=1)\n",
    "    double_code_data = news_data[news_data.countassetCodes == 2]\n",
    "    double_code_data_i = double_code_data.copy()\n",
    "    double_code_data_ii = double_code_data.copy()\n",
    "    if len(double_code_data_i) == 0:\n",
    "        double_code_data_i['assetCode'] = ''\n",
    "    else :\n",
    "        double_code_data_i['assetCode'] = double_code_data_i.apply(lambda x: x.assetCodes[0],axis=1)\n",
    "    if len(double_code_data_ii) == 0:\n",
    "        double_code_data_ii['assetCode'] = ''\n",
    "    else :\n",
    "        double_code_data_ii['assetCode'] = double_code_data_ii.apply(lambda x: x.assetCodes[1],axis=1)\n",
    "    unmapped_code_data = news_data[news_data.countassetCodes == 0]\n",
    "    if len(unmapped_code_data) == 0 :\n",
    "        unmapped_code_data['assetCode'] = ''\n",
    "    else :\n",
    "        unmapped_code_data['assetCode']  = unmapped_code_data.apply(lambda row: asset_name_code_map.get(row.assetName,''),axis=1)\n",
    "    #ADD ALL THE TICKERS CORRESPONDING TO ONE ASSET NAME\n",
    "    mapped_data = unmapped_code_data[unmapped_code_data.assetCode != '']\n",
    "    #Concatenate the data to make it complete\n",
    "    complete_news_data = pd.concat([single_code_data, double_code_data_i, double_code_data_ii, mapped_data],ignore_index=True)\n",
    "    complete_news_data.drop(columns=['assetCodes','countassetCodes', 'assetName'],inplace=True)\n",
    "    complete_news_data.drop_duplicates(inplace=True)\n",
    "    return complete_news_data\n",
    "\n",
    "news_data = preprocess_news_data(news_data)\n",
    "news_data = group_providers(news_data)\n",
    "complete_news_data = preprocess_asset_code_news_data(news_data,asset_code_name_map,asset_name_code_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Market Data Preprocessing\n",
    "def preprocess_market_data(market_data,is_test_data=False):\n",
    "    market_data[ 'returnsOpenClose'] = market_data.close/market_data.open -1\n",
    "    drop_columns = ['close','open','assetName']\n",
    "    if is_test_data == False:\n",
    "        drop_columns = drop_columns+ ['universe']\n",
    "    market_data.drop(columns=drop_columns,inplace=True)\n",
    "    market_data['time'] = pd.to_datetime(market_data['time']).dt.date.astype(str)\n",
    "    return market_data\n",
    "market_data = preprocess_market_data(market_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = market_data[market_data.returnsOpenNextMktres10 <= 1]\n",
    "clean_data = clean[clean.returnsOpenNextMktres10 >= -1]\n",
    "positive_data = clean_data[clean_data.returnsOpenNextMktres10>=0]\n",
    "clean_y = clean_data.returnsOpenNextMktres10\n",
    "np.quantile(clean_y,[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "#plt.hist(clean_data.returnsOpenNextMktres10,bins=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where modeling and calibration starts\n",
    "#Merging Market and News Data\n",
    "def merge_market_news_data(market_data,news_data):\n",
    "    combined_data = pd.merge(market_data,news_data,how='inner',left_on=['time','assetCode'], right_on = ['time','assetCode'])\n",
    "    #Two categorical variables: urgency,provider\n",
    "    combined_data['marketCommentary'] = combined_data['marketCommentary'].astype('int')\n",
    "    combined_hot_encoded = pd.get_dummies(combined_data,prefix=['urgency','provider'],columns=['urgency','provider'])\n",
    "    combined_hot_encoded.drop_duplicates(inplace=True)\n",
    "    combined_hot_encoded.dropna(inplace=True)\n",
    "    return combined_hot_encoded\n",
    "combined_data = merge_market_news_data(market_data,complete_news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = combined_data.iloc[:,3:].columns.tolist()\n",
    "combined_features = list(set(combined_features) - set(['returnsOpenNextMktres10']))\n",
    "combined_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_features = market_data.iloc[:,3:].columns.tolist()\n",
    "market_features = list(set(market_features) - set(['returnsOpenNextMktres10']))\n",
    "market_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a XGBoost regression just using market data\n",
    "def market_data_XGB(market_data,market_features):\n",
    "    market_data_copy = market_data.copy()\n",
    "    market_data_copy.dropna(inplace=True)\n",
    "    for c in market_features:\n",
    "        market_data_copy = market_data_copy[market_data_copy[c] < 1.0]\n",
    "        market_data_copy = market_data_copy[market_data_copy[c] > -1.0]\n",
    "    market_data_copy = market_data_copy[market_data_copy['returnsOpenNextMktres10'] < 1.0]\n",
    "    market_data_copy = market_data_copy[market_data_copy['returnsOpenNextMktres10'] > -1.0]\n",
    "    X_data = market_data_copy[market_features].values\n",
    "    y_data = market_data_copy[['returnsOpenNextMktres10']].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data ,test_size=0.3)\n",
    "    xgb = xgboost.XGBRegressor(n_estimators=300, learning_rate=0.1, gamma=0, subsample=0.75,\n",
    "                               colsample_bytree=1, max_depth=7)\n",
    "    xgb.fit(X_train,y_train,eval_metric='rmse',eval_set=[(X_test,y_test)],early_stopping_rounds=10)\n",
    "    return xgb\n",
    "market_XGB = market_data_XGB(market_data,market_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a XGBoost regression for combined data\n",
    "def combined_data_XGB(combined_data,combined_features):\n",
    "    combined_data_copy = combined_data.copy()\n",
    "    combined_data_copy.dropna(inplace=True)\n",
    "    for c in combined_features:\n",
    "        combined_data_copy = combined_data_copy[combined_data_copy[c] <= 1.0]\n",
    "        combined_data_copy = combined_data_copy[combined_data_copy[c] >= -1.0]\n",
    "    combined_data_copy = combined_data_copy[combined_data_copy['returnsOpenNextMktres10'] < 1.0]\n",
    "    combined_data_copy = combined_data_copy[combined_data_copy['returnsOpenNextMktres10'] > -1.0]\n",
    "    X_data = combined_data_copy[combined_features].values\n",
    "    y_data = combined_data_copy[['returnsOpenNextMktres10']].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data ,test_size=0.3)\n",
    "    xgb = xgboost.XGBRegressor(n_estimators=200, learning_rate=0.1, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "    xgb.fit(X_train,y_train,eval_metric='rmse',eval_set=[(X_test,y_test)],early_stopping_rounds=10)\n",
    "    return xgb\n",
    "\n",
    "combined_XGB = combined_data_XGB(combined_data,combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(market_test,news_test,asset_code_name_map,asset_name_code_map,combined_features):\n",
    "    market_test_data = preprocess_market_data(market_test,is_test_data=True)\n",
    "    news_test_data = preprocess_news_data(news_test)\n",
    "    news_test_data = group_providers(news_test_data)\n",
    "    complete_news_test_data = preprocess_asset_code_news_data(news_test_data,asset_code_name_map,asset_name_code_map)\n",
    "    combined_test_data = merge_market_news_data(market_test_data,complete_news_test_data)\n",
    "    present_columns = combined_test_data.columns\n",
    "    remaining_columns = list(set(combined_features) - set(present_columns))\n",
    "    for c in remaining_columns:\n",
    "        combined_test_data[c] = 0\n",
    "    return combined_test_data,market_test_data\n",
    "\n",
    "def calculate_weight(pred_return):\n",
    "    if pred_return < 0 :\n",
    "        abs_pred_return = -pred_return\n",
    "    else :\n",
    "        abs_pred_return = pred_return\n",
    "    if abs_pred_return < 0.1:\n",
    "        result = 1\n",
    "    else :\n",
    "        result = 0.1/abs_pred_return\n",
    "    return np.sign(pred_return)*result\n",
    "\n",
    "def assign_appropriate_weight(array):\n",
    "    result = np.zeros(array.shape)\n",
    "    for i in range(len(array)):\n",
    "        result[i] = calculate_weight(array[i])\n",
    "    return result\n",
    "\n",
    "def make_news_based_predictions(combined_test_data,combined_features,xgb):\n",
    "    combined_test_data['pred_return'] = 0\n",
    "    X_test_data = combined_test_data[combined_features].values\n",
    "    predictions = xgb.predict(X_test_data)\n",
    "    combined_test_data.pred_return = predictions\n",
    "    combined_submit_df = combined_test_data[['assetCode','pred_return']]\n",
    "    combined_submit_df = combined_submit_df.groupby(['assetCode'],as_index=False).agg([np.mean]).reset_index()\n",
    "    combined_submit_df.columns = [''.join(c) for c in combined_submit_df.columns]\n",
    "    pred_returns = combined_submit_df.pred_returnmean.values\n",
    "    weight_vector = assign_appropriate_weight(pred_returns)\n",
    "    combined_submit_df['confidenceValue'] = weight_vector\n",
    "    combined_submit_df = combined_submit_df[['assetCode','confidenceValue']]\n",
    "    return combined_submit_df\n",
    "\n",
    "#combined_test_data,market_test_data = process_test_data(market_test,news_test,asset_code_name_map,asset_name_code_map,combined_features)\n",
    "#news_submission_df = make_news_based_predictions(combined_test_data,combined_features,combined_XGB)\n",
    "#assets_predicted = list(news_submission_df.assetCode)\n",
    "#relevant_market_test_data = market_test_data[~market_test_data['assetCode'].isin(assets_predicted)]\n",
    "#market_submission_df = make_news_based_predictions(relevant_market_test_data,market_features,market_XGB)\n",
    "#submit_df = pd.concat([news_submission_df,market_submission_df],ignore_index=True)\n",
    "\n",
    "#print( 'size matches as expected : ', len(submit_df) == len(predictions_template_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for (market_test, news_test, predictions_template_df) in days:\n",
    "    combined_test_data,market_test_data = process_test_data(market_test,news_test,asset_code_name_map,asset_name_code_map,combined_features)\n",
    "    news_submission_df = make_news_based_predictions(combined_test_data,combined_features,combined_XGB)\n",
    "    assets_predicted = list(news_submission_df.assetCode)\n",
    "    relevant_market_test_data = market_test_data[~market_test_data['assetCode'].isin(assets_predicted)]\n",
    "    market_submission_df = make_news_based_predictions(relevant_market_test_data,market_features,market_XGB)\n",
    "    submit_df = pd.concat([news_submission_df,market_submission_df],ignore_index=True)\n",
    "    print( 'size matches as expected : ', len(submit_df) == len(predictions_template_df))\n",
    "    env.predict(submit_df)\n",
    "    print( \"prediction complete for day : \", count)\n",
    "    count = count+1\n",
    "env.write_submission_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
