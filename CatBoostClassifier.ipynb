{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kaggle.competitions import twosigmanews\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import CatBoostClassifier\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = twosigmanews.make_env()\n",
    "(market_data_orig, news_data_orig) = env.get_training_data()\n",
    "\n",
    "def process_cols(df):\n",
    "    for column in df.select_dtypes(include='float64').columns:\n",
    "        df[column] = df[column].astype('float16')\n",
    "\n",
    "    for column in df.select_dtypes(include='int64').columns:\n",
    "        df[column] = df[column].astype('int16')\n",
    "    \n",
    "    return df\n",
    "\n",
    "market_data_orig = process_cols(market_data_orig)\n",
    "news_data_orig = process_cols(news_data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data = market_data_orig.copy()\n",
    "news_data = news_data_orig.copy()\n",
    "market_data['time2'] = [int(x[0:4]) for x in market_data.time.astype(str)]\n",
    "news_data['time2'] = [int(x[0:4]) for x in news_data.time.astype(str)]\n",
    "years = market_data.time2.unique()\n",
    "print(years)\n",
    "n = len(years)\n",
    "#removing 2007 and 2008 data\n",
    "market_data = market_data[market_data['time2']>2009]\n",
    "news_data = news_data[news_data['time2']>2009]\n",
    "market_data.drop(columns=['time2'], inplace = True)\n",
    "news_data.drop(columns=['time2'], inplace = True)\n",
    "print(market_data.shape)\n",
    "print(news_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping the originals as is\n",
    "#market_data = market_data_orig.iloc[0:1000000,]\n",
    "#news_data = news_data_orig.iloc[0:2000000,]\n",
    "asset_label = {k: v for v, k in enumerate(market_data['assetCode'].unique())}\n",
    "asset_code_name_map = market_data.set_index('assetCode').to_dict()['assetName']\n",
    "asset_name_code_map = market_data.set_index('assetName').to_dict()['assetCode']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Level 1\n",
    "def preprocess_news_data(news_data):\n",
    "    news_data['firstMentionLoc'] = 1- news_data['firstMentionSentence']/news_data['sentenceCount']\n",
    "    news_data['sentimentWordFraction'] = news_data['sentimentWordCount']/news_data['wordCount']\n",
    "    news_data = news_data.drop(columns = ['noveltyCount12H', 'noveltyCount24H',\n",
    "                                          'noveltyCount3D', 'noveltyCount5D',\n",
    "                                          'noveltyCount7D', 'volumeCounts12H',\n",
    "                                          'volumeCounts24H', 'volumeCounts3D',\n",
    "                                          'volumeCounts5D', 'volumeCounts7D',\n",
    "                                          'bodySize', 'audiences', 'subjects',\n",
    "                                          'takeSequence', 'headline', 'sourceId',\n",
    "                                          'firstCreated', 'sourceTimestamp',\n",
    "                                          'firstMentionSentence', 'sentenceCount',\n",
    "                                          'sentimentWordCount', 'wordCount','companyCount',\n",
    "                                          'sentimentClass','headlineTag' ])\n",
    "#Think about adding sentimentClass,headline and using it as a categorical variable\n",
    "    news_data['time'] = pd.to_datetime(news_data['time']).dt.date.astype(str)\n",
    "    news_data = news_data.groupby(['time','urgency', 'provider',\n",
    "                                   'marketCommentary', 'assetCodes','assetName'\n",
    "                                  ],as_index=False).agg([np.mean,np.min,np.max]).reset_index()\n",
    "    news_data.columns = [''.join(c) for c in news_data.columns]\n",
    "    news_data['assetCodes'] = news_data['assetCodes'].astype(str)\n",
    "    news_data['assetCodes'] = news_data['assetCodes'].map(lambda x: eval(x.replace('{','[').replace('}',']')))\n",
    "    return news_data\n",
    "\n",
    "def group_providers(news_data):\n",
    "    provider_data = pd.DataFrame(news_data.provider.value_counts())\n",
    "    provider_data.reset_index(inplace=True)\n",
    "    provider_data['map'] = provider_data['index'].astype(str)\n",
    "    provider_data.loc[provider_data.provider < provider_data['provider'][2], 'map'] = 'other'\n",
    "    provider_data.drop(columns=['provider'],inplace=True)\n",
    "    news_data = pd.merge(news_data,provider_data,how='left',left_on='provider', right_on='index')\n",
    "    news_data.provider = news_data.map\n",
    "    news_data.drop(columns=['index','map'],inplace=True)\n",
    "    return news_data\n",
    "\n",
    "def preprocess_asset_code_news_data(news_data,asset_code_name_map,asset_name_code_map):\n",
    "    #Preprocessing Level 2\n",
    "    news_data['provider'] = news_data['provider'].map(prov_label)\n",
    "    news_data['assetCodes'] = news_data.apply(lambda row : [c for c in row.assetCodes if c in asset_code_name_map],axis=1)\n",
    "    news_data['countassetCodes'] = news_data.apply(lambda row: len(row.assetCodes), axis=1)\n",
    "    single_code_data = news_data[news_data.countassetCodes == 1]\n",
    "    single_code_data['assetCode'] = single_code_data.apply(lambda x: x.assetCodes[0],axis=1)\n",
    "    double_code_data = news_data[news_data.countassetCodes == 2]\n",
    "    double_code_data_i = double_code_data.copy()\n",
    "    double_code_data_ii = double_code_data.copy()\n",
    "    if len(double_code_data_i) == 0:\n",
    "        double_code_data_i['assetCode'] = ''\n",
    "    else :\n",
    "        double_code_data_i['assetCode'] = double_code_data_i.apply(lambda x: x.assetCodes[0],axis=1)\n",
    "    if len(double_code_data_ii) == 0:\n",
    "        double_code_data_ii['assetCode'] = ''\n",
    "    else :\n",
    "        double_code_data_ii['assetCode'] = double_code_data_ii.apply(lambda x: x.assetCodes[1],axis=1)\n",
    "    unmapped_code_data = news_data[news_data.countassetCodes == 0]\n",
    "    if len(unmapped_code_data) == 0 :\n",
    "        unmapped_code_data['assetCode'] = ''\n",
    "    else :\n",
    "        unmapped_code_data['assetCode']  = unmapped_code_data.apply(lambda row: asset_name_code_map.get(row.assetName,''),axis=1)\n",
    "    #ADD ALL THE TICKERS CORRESPONDING TO ONE ASSET NAME\n",
    "    mapped_data = unmapped_code_data[unmapped_code_data.assetCode != '']\n",
    "    #Concatenate the data to make it complete\n",
    "    complete_news_data = pd.concat([single_code_data, double_code_data_i, double_code_data_ii, mapped_data],ignore_index=True)\n",
    "    complete_news_data.drop(columns=['assetCodes','countassetCodes', 'assetName'],inplace=True)\n",
    "    complete_news_data.drop_duplicates(inplace=True)\n",
    "    return complete_news_data\n",
    "\n",
    "news_data = preprocess_news_data(news_data)\n",
    "news_data = group_providers(news_data)\n",
    "prov_label = {k: v for v, k in enumerate(news_data['provider'].unique())}\n",
    "complete_news_data = preprocess_asset_code_news_data(news_data,asset_code_name_map,asset_name_code_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Market Data Preprocessing\n",
    "def preprocess_market_data(market_data,is_test_data=False):\n",
    "    market_data['assetCodeT'] = market_data['assetCode'].map(asset_label)\n",
    "    market_data[ 'returnsOpenClose'] = market_data.close/market_data.open -1\n",
    "    drop_columns = ['close','open','assetName']\n",
    "    if is_test_data == False:\n",
    "        drop_columns = drop_columns+ ['universe']\n",
    "    market_data.drop(columns=drop_columns,inplace=True)\n",
    "    market_data['time'] = pd.to_datetime(market_data['time']).dt.date.astype(str)\n",
    "    return market_data\n",
    "market_data = preprocess_market_data(market_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data):\n",
    "    for c in data.columns.values[0:12]:\n",
    "        new_c = c + \"_sq\"\n",
    "        data[new_c] = data[c]**2\n",
    "        data = process_cols(data)\n",
    "#        new_c = c + \"_log\"\n",
    "#        minv = min(data[c])\n",
    "#        data[new_c] = np.log(data[c]-minv+1)\n",
    "    return data\n",
    "\n",
    "def moving_avg(data, freq):\n",
    "    ma = data.rolling(freq).mean()\n",
    "    ma = ma.fillna(0)\n",
    "    ma = ma.rename(lambda x: x + \"_MA\", axis=1)\n",
    "    ma = process_cols(ma)\n",
    "#    ma.drop(columns = ma.columns.values[60:])\n",
    "    return(ma)\n",
    "\n",
    "def market_data_enhance(market_data):\n",
    "    time = market_data['time']\n",
    "    assetCode = market_data['assetCode']\n",
    "    assetCodeT = market_data['assetCodeT']\n",
    "    try:\n",
    "        returnsOpenNextMktres10 = market_data['returnsOpenNextMktres10']\n",
    "        market_data.drop(columns=['time', 'assetCode', 'returnsOpenNextMktres10', 'assetCodeT'],\n",
    "                         inplace=True)\n",
    "    except:\n",
    "        market_data.drop(columns=['time', 'assetCode', 'assetCodeT'], inplace=True)\n",
    "    market_data_ma = moving_avg(market_data, 10)\n",
    "    market_data = market_data.join(market_data_ma, lsuffix = '', rsuffix = '')\n",
    "    market_data = add_features(market_data)\n",
    "    market_data['time'] = time\n",
    "    market_data['assetCode'] = assetCode\n",
    "    market_data['assetCodeT'] = assetCodeT\n",
    "    try:\n",
    "        market_data['returnsOpenNextMktres10'] = returnsOpenNextMktres10\n",
    "    except:\n",
    "        dummy = 2\n",
    "        \n",
    "    return market_data\n",
    "\n",
    "market_data = market_data_enhance(market_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where modeling and calibration starts\n",
    "#Merging Market and News Data\n",
    "def merge_market_news_data(market_data,news_data):\n",
    "    combined_data = pd.merge(market_data,news_data,how='inner',left_on=['time','assetCode'], right_on = ['time','assetCode'])\n",
    "    #Two categorical variables: urgency,provider\n",
    "    combined_data['marketCommentary'] = combined_data['marketCommentary'].astype('int')\n",
    "#    combined_hot_encoded = pd.get_dummies(combined_data,prefix=['urgency','provider'],columns=['urgency','provider'])\n",
    "    combined_data.drop_duplicates(inplace=True)\n",
    "    combined_data.dropna(inplace=True)\n",
    "    return combined_data\n",
    "combined_data = merge_market_news_data(market_data,complete_news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = combined_data.columns.values\n",
    "combined_features = list(set(combined_features) - set(['returnsOpenNextMktres10', 'assetCode', 'time']))\n",
    "combined_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_features = market_data.columns.tolist()\n",
    "market_features = list(set(market_features) - set(['returnsOpenNextMktres10', 'assetCode', 'time']))\n",
    "market_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_data_XGB(market_data,market_features):\n",
    "#    market_data_copy = market_data.copy()\n",
    "#    market_data_copy.dropna(inplace=True)\n",
    "    print(market_data.shape)\n",
    "#    for c in market_features:\n",
    "#        market_data_copy[c].fillna(np.nanmedian(market_data_copy[c]), inplace=True)\n",
    "#        market_data_copy = market_data_copy[market_data_copy[c] < 1.0]\n",
    "#        market_data_copy = market_data_copy[market_data_copy[c] > -1.0]\n",
    "#    print(market_data.shape)\n",
    "    market_data = market_data[market_data['returnsOpenNextMktres10'] < 1.0]\n",
    "    market_data = market_data[market_data['returnsOpenNextMktres10'] > -1.0]\n",
    "    print(market_data.shape)\n",
    "    y_data = market_data[['returnsOpenNextMktres10']].values\n",
    "    market_data = market_data[market_features].values\n",
    "    print(y_data.shape)\n",
    "#    x_train, x_test, y_train, y_test = train_test_split(X_data, y_data ,test_size=0.2)\n",
    "#    for df in [x_train, x_test, y_train, y_test]:\n",
    "#    for df in [market_data, y_data]:\n",
    "    col_mean = np.nanmedian(market_data, axis=0)\n",
    "    inds = np.where(np.isnan(market_data))\n",
    "    market_data[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "#    x_train = x_train.astype(np.float32)\n",
    "#    x_test = x_test.astype(np.float32)\n",
    "    y_data = y_data.flatten().astype(np.float32)\n",
    "    y_data = (y_data>0).astype(int)\n",
    "#    y_test = y_test.flatten().astype(np.float32)\n",
    "    a = market_features.index('assetCodeT')\n",
    "    \n",
    "    cb = CatBoostClassifier(thread_count=4, n_estimators=150, max_depth=8, \n",
    "                               eta=0.1, loss_function='Logloss' , verbose=15)\n",
    "    cb.fit(market_data, y_data,\n",
    "#             eval_set=(x_test,y_test),\n",
    "#             use_best_model=True,\n",
    "             cat_features = [a],\n",
    "             verbose=20)\n",
    "    return cb\n",
    "market_XGB = market_data_XGB(market_data,market_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a XGBoost regression for combined data\n",
    "def combined_data_XGB(combined_data,combined_features):\n",
    "#    combined_data_copy = combined_data.copy()\n",
    "#    combined_data_copy.dropna(inplace=True)\n",
    "#    for c in combined_features:\n",
    "#        combined_data_copy[c].fillna(np.nanmean(combined_data_copy[c]), inplace=True)\n",
    "#        combined_data_copy = combined_data_copy[combined_data_copy[c] <= 1.0]\n",
    "#        combined_data_copy = combined_data_copy[combined_data_copy[c] >= -1.0]\n",
    "    combined_data = combined_data[combined_data['returnsOpenNextMktres10'] < 1.0]\n",
    "    combined_data = combined_data[combined_data['returnsOpenNextMktres10'] > -1.0]\n",
    "    print(combined_data.columns.values)\n",
    "    X_data = combined_data[combined_features].values\n",
    "    y_data = combined_data[['returnsOpenNextMktres10']].values\n",
    " #   x_train, x_test, y_train, y_test = train_test_split(X_data, y_data ,test_size=0.3)\n",
    "#    print(y_train.shape)\n",
    "#    print(y_test.shape)\n",
    "\n",
    "#    for df in [x_train, x_test, y_train, y_test]:\n",
    "    for df in [X_data, y_data]:\n",
    "        col_mean = np.nanmedian(df, axis=0)\n",
    "        inds = np.where(np.isnan(df))\n",
    "        df[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "#    x_train = x_train.astype(np.float32)\n",
    "#    x_test = x_test.astype(np.float32)\n",
    "    y_data = y_data.flatten().astype(np.float32)\n",
    "    y_data = (y_data>0).astype(int)\n",
    "\n",
    "    a = combined_features.index('assetCodeT')\n",
    "    b = combined_features.index('provider')\n",
    "    c = combined_features.index('urgency')\n",
    "\n",
    "    cb = CatBoostClassifier(thread_count=4, n_estimators=150, max_depth=8, \n",
    "                               eta=0.1, loss_function='Logloss' , verbose=15)\n",
    "    cb.fit(X_data, y_data,\n",
    "#    cb.fit(x_train, y_train,\n",
    "#             eval_set=(x_test,y_test),\n",
    "#             use_best_model=True,\n",
    "             cat_features = [a,b,c],\n",
    "             verbose=20)\n",
    "    return cb\n",
    "\n",
    "combined_XGB = combined_data_XGB(combined_data,combined_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()\n",
    "def process_test_data(market_test,news_test,asset_code_name_map,asset_name_code_map,combined_features):\n",
    "    market_test_data = preprocess_market_data(market_test,is_test_data=True)\n",
    "    market_test_data = market_data_enhance(market_test_data)\n",
    "    news_test_data = preprocess_news_data(news_test)\n",
    "    news_test_data = group_providers(news_test_data)\n",
    "    complete_news_test_data = preprocess_asset_code_news_data(news_test_data,asset_code_name_map,asset_name_code_map)\n",
    "    combined_test_data = merge_market_news_data(market_test_data,complete_news_test_data)\n",
    "    present_columns = combined_test_data.columns\n",
    "    remaining_columns = list(set(combined_features) - set(present_columns))\n",
    "    for c in remaining_columns:\n",
    "        combined_test_data[c] = 0\n",
    "    return combined_test_data,market_test_data\n",
    "\n",
    "def calculate_weight(pred_return):\n",
    "    if pred_return < 0 :\n",
    "        abs_pred_return = -pred_return\n",
    "    else :\n",
    "        abs_pred_return = pred_return\n",
    "    if abs_pred_return < 0.1:\n",
    "        result = 1\n",
    "    else :\n",
    "        result = 0.1/abs_pred_return\n",
    "    return np.sign(pred_return)*result\n",
    "\n",
    "def assign_appropriate_weight(array):\n",
    "    result = np.zeros(array.shape)\n",
    "    for i in range(len(array)):\n",
    "        result[i] = calculate_weight(array[i])\n",
    "    return result\n",
    "\n",
    "def make_news_based_predictions(combined_test_data,combined_features,xgb):\n",
    "    combined_test_data['pred_return'] = 0\n",
    "    X_test_data = combined_test_data[combined_features].values\n",
    "    col_mean = np.nanmedian(X_test_data, axis=0).astype(int)\n",
    "    inds = np.where(np.isnan(X_test_data))\n",
    "    X_test_data[inds] = np.take(col_mean, inds[1])\n",
    "    predictions = 2*xgb.predict_proba(X_test_data)[:,1] - 1\n",
    "    combined_test_data.pred_return = predictions\n",
    "    combined_submit_df = combined_test_data[['assetCode','pred_return']]\n",
    "    combined_submit_df = combined_submit_df.groupby(['assetCode'],as_index=False).agg([np.mean]).reset_index()\n",
    "    combined_submit_df.columns = [''.join(c) for c in combined_submit_df.columns]\n",
    "    pred_returns = combined_submit_df.pred_returnmean.values\n",
    "#    weight_vector = assign_appropriate_weight(pred_returns)\n",
    "    combined_submit_df['confidenceValue'] = pred_returns #weight_vector\n",
    "    combined_submit_df = combined_submit_df[['assetCode','confidenceValue']]\n",
    "    return combined_submit_df\n",
    "\n",
    "#combined_test_data,market_test_data = process_test_data(market_test,news_test,asset_code_name_map,asset_name_code_map,combined_features)\n",
    "#news_submission_df = make_news_based_predictions(combined_test_data,combined_features,combined_XGB)\n",
    "#assets_predicted = list(news_submission_df.assetCode)\n",
    "#relevant_market_test_data = market_test_data[~market_test_data['assetCode'].isin(assets_predicted)]\n",
    "#market_submission_df = make_news_based_predictions(relevant_market_test_data,market_features,market_XGB)\n",
    "#submit_df = pd.concat([news_submission_df,market_submission_df],ignore_index=True)\n",
    "\n",
    "#print( 'size matches as expected : ', len(submit_df) == len(predictions_template_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
